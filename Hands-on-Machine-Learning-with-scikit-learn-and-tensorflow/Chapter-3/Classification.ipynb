{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Machine Learning with Scikit-Learn and Tensorflow\n",
    "\n",
    "## Chapter 3: Classification\n",
    "\n",
    "### Notebook by: Arunava\n",
    "\n",
    "<hr/>\n",
    "<hr/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring Warnings\n",
    "import warnings\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist['data'], mnist['target']\n",
    "print (X.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X[23456].reshape(28, 28)\n",
    "plt.imshow(some_digit, cmap=matplotlib.cm.binary, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print ('Label: ', y[23456])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_3 = (y_train == 3)\n",
    "y_test_3 = (y_test == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_3)\n",
    "sgd_clf.predict(some_digit.ravel().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing cross-validation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits = 3, random_state=42)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_3):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    \n",
    "    X_train_f = X_train[train_index]\n",
    "    y_train_f = y_train_3[train_index]\n",
    "    \n",
    "    X_test_f = X_train[test_index]\n",
    "    y_test_f = y_train_3[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_f, y_train_f)\n",
    "    pred = clone_clf.predict(X_test_f)\n",
    "    \n",
    "    n_correct = sum(pred == y_test_f)\n",
    "    acc = n_correct/len(X_test_f)\n",
    "    print ('Accuracy - Iteration', i, ' :', acc)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `cross_val_score()` along with `sgd_clf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_3, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a dumb classifier that predicts everything in the 'not 3' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never3Clasifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X, y=None):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_3_clf = Never3Clasifier()\n",
    "cross_val_score(never_3_clf, X_train, y_train_3, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~90% Accuaracy!\n",
    "\n",
    "This is simply because about 10% of the classes are `3` and rest are `not_3`.\n",
    "\n",
    "Also, if you simply predict not `3` always then you are bound to get accuracy of `90%` all the time as only `10%` of the examples are `3`.\n",
    "\n",
    "This is the reason that `accuracy` is not a very good measure of how good a classifier really is. (Especially in cases of skewed datasets where some classes are much more frequent than others)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A general idea is to count the number of times instances of class A were classified as class B and class A i.e. correct predictions and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_3, cv=3)\n",
    "confusion_matrix(y_train_pred, y_train_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the confusion matrix, each **row represents an actual class** while each **columnrepresents a predicted class**.\n",
    "\n",
    "1, 1 -> true negetives\n",
    "\n",
    "1, 2 -> false positives\n",
    "\n",
    "2, 1 -> false negetives\n",
    "\n",
    "2, 2 -> true positives\n",
    "\n",
    "A perfect classifier would have only the true positives and true negetives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more consise metric would be look at the classifier's precision which is the accuary of the positive predictions.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "precision = \\frac{TP}{TP+FP}\n",
    "\\end{align}\n",
    "\n",
    "where, _TP_ is the number of true positives and _FP_ is the number of false positives\n",
    "\n",
    "A trivial way to have a perfect precision is to make one single positive prediction and ensure it is correct (`precision` = `1/1` = `100%`)\n",
    "\n",
    "This would not be very useful.\n",
    "\n",
    "So, precision is typically used along with another metric name _recall_ (sensitivity or true positive rate (TPR)) -> this is the ratio of positive instances that are correctly detected by the classifier\n",
    "\n",
    "\\begin{align}\n",
    "recall = \\frac{TP}{TP+FN}\n",
    "\\end{align}\n",
    "\n",
    "where _FN_ is false negetives\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print ('Precision score:', precision_score(y_train_3, y_train_pred))\n",
    "print ('Recall score:', recall_score(y_train_3, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the 3-detector doesn't seem as good as previous.\n",
    "\n",
    "As you can see it is correct only `82%` of the time (precision score)\n",
    "\n",
    "and, it detects only `80%` of the `5`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often combine precision and recall into a single metric called the _F1 score_.\n",
    "\n",
    "It calculates the harmonic mean which gives more weight to the low values (irrespective to regular mean where all values are treated equally)\n",
    "\n",
    "\\begin{align}\n",
    "F1 = \\frac{TP}{TP+\\frac{FN+FP}{2}}\n",
    "\\end{align}\n",
    "\n",
    "Okay, lets now compute the f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_3, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score favours classifiers that have similar precision and recall. This is not always the case as in some context you mostly care about the precision and in other contexts you care about the recall.\n",
    "\n",
    "Increasing precision reduces recall and vice versa. This is called **precision/recall tradeoff**.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/Recall Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SDGClassifier you cannot set the threshold directly but you can get the decision scores which is calcualted for each instance.\n",
    "\n",
    "Raising the threshold increases the precision and decreases the recall\n",
    "\n",
    "Lowering the threshold decreases the precision and increases the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function([some_digit.ravel()])\n",
    "print ('y_scores: ', y_scores)\n",
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "print ('y_some_digit_pred: ', y_some_digit_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the threshold is set to `0` every instance is classified as `5` and thus whatever the score is the classifier would return `True`.\n",
    "\n",
    "Let's raise the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 230000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "print ('y_some_digit_pred: ', y_some_digit_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how can we decide what threshold to use?\n",
    "\n",
    "Let's get the scores of all instances using `cross_val_predict()`.\n",
    "\n",
    "Note: Since we need to get the scores we need to pass `method='decision_function'` to `cross_val_predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_3, cv=3, method='decision_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the scores we can now calculate `precision` and `recall` for thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_3, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], 'b-', label='Precision')\n",
    "    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.ylim([-0.2, 1.2])\n",
    "    \n",
    "plot_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precisions, recalls, 'b-')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recalls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we closely look at the precison vs threshold graph and find that to get a `~90%` precision we need to use a `threshold` of `70000` then we can just simply make the predictions as the following without the use of the `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_90_pred = (y_scores > 70000)\n",
    "print ('Precision score:', precision_score(y_train_3, y_train_90_pred))\n",
    "print ('Recall score:', recall_score(y_train_3, y_train_90_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! we now have a `91%` precision classifier. It's pretty easy to create a classifier with any precision.\n",
    "\n",
    "But, it comes with a cost, look at the recall score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ROC Curve\n",
    "\n",
    "The receiver operating characteristic (ROC) curve is another common tool with binary classifiers.\n",
    "\n",
    "It plots the TPR (true positive rate) against the FPR (false positive rate) i.e. `sensitivity` vs `1-specificity`.\n",
    "\n",
    "Where \n",
    "\n",
    "\\begin{align}\n",
    "TPR = sensitivity = recall\n",
    "\\end{align}\n",
    "\n",
    "And,\n",
    "\n",
    "\\begin{align}\n",
    "FPR = 1 - TNR\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "FPR = false positive rate\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "TNR = specificity\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_3, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, there is a trade-off between the TPR and FPR.\n",
    "\n",
    "One way to compare classifiers is to measure the area under the curve(AUC). A perfect classifier will have a ROC AUC equal to `1`, whereas a purely randomly classifier will have a ROC AUC equal to `0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_3, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, we should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negetives. And the ROC curve otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a `RandomForestClassifier` and compare its ROC curve and the ROC AUC score to the `SDGClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_3, cv=3, method='predict_proba')\n",
    "y_probas_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since to plot ROC curve we need the scores and probabilities, a simple solution is to use the positive class's probability as the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1]\n",
    "fpr_f, tpr_f, thresholds_f = roc_curve(y_train_3, y_scores_forest)\n",
    "\n",
    "plt.plot(fpr, tpr, 'b:', label='SGD')\n",
    "plot_roc_curve(fpr_f, tpr_f, 'Random Forest')\n",
    "plt.legend(loc='bottom right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `RandomForestClassifier`'s ROC curve looks much better than `SGDClassifier`'s.\n",
    "\n",
    "And if we take a look at its roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('ROC AUC Score:', roc_auc_score(y_train_3, y_scores_forest))\n",
    "\n",
    "forest_clf.fit(X_train, y_train_3)\n",
    "print ('Precision Score: ', precision_score(y_train_3, forest_clf.predict(X_train)))\n",
    "print ('Recall Score: ', recall_score(y_train_3, forest_clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "So, now we know how to train binary classifiers:\n",
    "\n",
    "1) Choose appropriate metric for your task\n",
    "\n",
    "2) Evaluate your classifiers using cross validation\n",
    "\n",
    "3) Select precision/recall tradeoff that fits the needs\n",
    "\n",
    "4) Compare various models using ROC curves and ROC AUC scores.\n",
    "\n",
    "Now, let's try to detect more than `5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "Some algorithms are capable of handling multiple classes directly. Others are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification using multiple binary classifiers:\n",
    "\n",
    "1) OvA (one-versus-all)\n",
    "\n",
    "2) OvO (one-versus-one)\n",
    "\n",
    "One of the advantages of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.\n",
    "\n",
    "Scikit-Learn detects when we try to use a binary classifier for a multiclass classification task and it automatically runs OvA (except for SVM classifiers, which uses OvO).\n",
    "\n",
    "Let's try this with SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said, under the hood, scikit learn uses OvA and trains `10` different classifiers and makes a predictions based on their decision scores.\n",
    "\n",
    "To check this is the case, we can use the `decision_function()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = sgd_clf.decision_function([some_digit.ravel()])\n",
    "some_digit_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have `10` different classifiers each providing a decision scores. The highest score is that of the class `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.classes_[np.argmax(some_digit_scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to force scikit-learn to use `one-versus-one` or `one-versus-all` you can use the `OneVsOneClassifier` or `OneVsRestClassifier` classes. Simply create an instance and pass a binary classifier to its contructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "ovo_clf.predict([some_digit.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ovo_clf.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a RandomForestClassifier is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time sklearn doesn't uses OvA or OvO cause RandomForestClassifier directly classify instances into multiple classes. We can use `.predict_proba()` to get the list of probabilities that the classifier assigned to each instance for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.predict_proba([some_digit.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the SGDClassifier now using `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets over 83% on all folds. Simply scaling the inputs would improve the accuracy over 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=4, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WooHoo! Over `90%` accuracy!\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's start by looking at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an image representation of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most images are on the main diagonal which means that they were classified correctly. The 5s look a slightly darker which could mean that there are fewer images of 5s in the dataset or that the classifier does not perform as well on 5s as on others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fill the diagonal with zeros to keep only the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing individual errors can also be a good way to gain insights on what your classifier is doing and why it has errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(X, img_per_row):\n",
    "    '''fig, ax = plt.subplots(nrows=img_per_row, ncols=len(X)//img_per_row)\n",
    "    i = 0\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            col.imshow(X[i].reshape(28, 28), cmap=matplotlib.cm.binary)\n",
    "            i += 1\n",
    "    '''\n",
    "    plt.figure()\n",
    "    for i in range(len(X)):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap=matplotlib.cm.binary)\n",
    "    \n",
    "cl_a, cl_b = 3, 5\n",
    "\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(221); plot_digits(X_aa[:25], img_per_row=5)\n",
    "plt.subplot(222); plot_digits(X_ab[:25], img_per_row=5)\n",
    "plt.subplot(223); plot_digits(X_ba[:25], img_per_row=5)\n",
    "plt.subplot(224); plot_digits(X_bb[:25], img_per_row=5)\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the images a human would also go wrong, for others it seems like a obvious mistake and the reason is that we used a SGDClassifier whcih is a linear model.\n",
    "\n",
    "So, this classifier is quite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion would be to center the image and make sure its not too rotated.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Classification\n",
    "\n",
    "Until now each instance has always been classified to just one class. In some cases we may want a classifier to output multiple labels for each instance. \n",
    "\n",
    "An example would be to output multiple binary labels (such a system is called multilabel classification system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KNeighborsClassifier` supports multilabel classification and we train it using multiple labels.\n",
    "\n",
    "Now, let's make a perdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict([some_digit.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it gets it right!! The digit is `3` so its less than `7` indicated by the `False` and its odd indicated by `True`.\n",
    "\n",
    "There are many ways to evaluate a multilabel classifier and selecting the right metric really depends on your project. One approach is to measure the F1 score for each label and compute the average. \n",
    "\n",
    "Following computes F1 score across all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)\n",
    "#f1_score(y_train, y_train_knn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumes that all labels are equally important, which may not be the case.\n",
    "\n",
    "In case, we want to give each label a weight based on its support (the number of instances with that target label) then we can just set `average='weighted'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multioutput Classification\n",
    "\n",
    "Multioutput-multilabel classification (or simply multioutput classification) is a generalization of multilabel classification where each label can be multiclass.\n",
    "\n",
    "To illustrate this, we will build a system that removes noise from images. So, the output will be an array of pixel intensities (multilabel) and each pixel can have multiple values (multioutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "\n",
    "noise_tr = rnd.randint(0, 100, (len(X_train), 784))\n",
    "noise_tt = rnd.randint(0, 100, (len(X_test), 784))\n",
    "X_train_mod = X_train + noise_tr\n",
    "X_test_mod = X_test + noise_tt\n",
    "y_train_mod = X_train\n",
    "y_test_mod = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at an image from both the train set and test set (note this is not recommended to do as this can introduce snooping bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train_mod[23].reshape(28, 28), cmap=matpltolib.cm.binary)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X_test_mod[23].reshape(28, 28), cmap=matpltolib.cm.binary)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left is the noisy image and on the right is the clean target image. Now let's train the classifier and make it a clean image :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[23]])\n",
    "\n",
    "plt.imshow(clean_digit.reshape(28, 28), cmap=matplotlib.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks close to the target :)\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And we now know how to select good metrics for classification tasks, pick the appropriate precision/recall trade off, compare classifiers and more generally build good classification systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
